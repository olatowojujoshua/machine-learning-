{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad5bf006",
   "metadata": {},
   "source": [
    "\n",
    "# Module 2: Classification (Supervised Learning)\n",
    "\n",
    "**Learning Objectives**\n",
    "- Explain supervised learning for classification and contrast it with regression.\n",
    "- Understand logistic regression and decision tree criteria with clearly defined terms.\n",
    "- Train and interpret Logistic Regression, Decision Tree, and Random Forest on a small dataset.\n",
    "- Read diagnostics such as accuracy, classification report, predicted probabilities, and feature importances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56df8c9c",
   "metadata": {},
   "source": [
    "\n",
    "## Supervised Learning Recap\n",
    "\n",
    "In **supervised learning**, we have inputs (features) and outputs (labels). The model learns a mapping from inputs to outputs from labeled examples. In **classification**, labels are **categorical** (e.g., species A, B, or C). In **regression**, labels are **numeric**.\n",
    "\n",
    "We will work with the classic **Iris** dataset to build and interpret classification models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d147621c",
   "metadata": {},
   "source": [
    "\n",
    "## Logistic Regression: Model and Terms (Multiclass via Softmax)\n",
    "\n",
    "For **binary classification**, logistic regression models the probability of the positive class with the **sigmoid**:\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}, \\quad p(y=1 \\mid x) = \\sigma(\\theta^\\top x).\n",
    "$$\n",
    "\n",
    "**Terms**\n",
    "- $x \\in \\mathbb{R}^d$: feature vector (input).\n",
    "- $\\theta \\in \\mathbb{R}^d$: model parameters (weights) learned from data.\n",
    "- $z = \\theta^\\top x$: linear score.\n",
    "- $p(y=1 \\mid x)$: predicted probability of class \\(1\\) given features \\(x\\).\n",
    "- $\\sigma(\\cdot)$: sigmoid function mapping any real number to \\([0,1]\\).\n",
    "\n",
    "For **multiclass** with classes \\(k \\in \\{1,\\dots,K\\}\\), use **softmax**:\n",
    "$$\n",
    "p(y = k \\mid x) = \\frac{e^{\\theta_k^\\top x}}{\\sum_{j=1}^{K} e^{\\theta_j^\\top x}}.\n",
    "$$\n",
    "- $\\theta_k$: parameters for class \\(k\\).\n",
    "- Denominator ensures probabilities are nonnegative and sum to \\(1\\).\n",
    "\n",
    "**Decision rule (binary, with threshold \\(\\tau\\))**\n",
    "$$\n",
    "\\hat{y} = \\mathbb{1}\\!\\left[p(y=1 \\mid x) \\ge \\tau\\right], \\quad \\text{commonly } \\tau = 0.5.\n",
    "$$\n",
    "\n",
    "Parameters \\(\\theta\\) are typically found by minimizing **log-loss** with regularization to reduce overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ebeb96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "-------------------\n",
      "Accuracy: 0.921\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        12\n",
      "  versicolor       0.86      0.92      0.89        13\n",
      "   virginica       0.92      0.85      0.88        13\n",
      "\n",
      "    accuracy                           0.92        38\n",
      "   macro avg       0.92      0.92      0.92        38\n",
      "weighted avg       0.92      0.92      0.92        38\n",
      "\n",
      "Predicted probabilities for first 5 test samples:\n",
      "Sample 0: setosa=0.985, versicolor=0.015, virginica=0.000\n",
      "Sample 1: setosa=0.034, versicolor=0.621, virginica=0.345\n",
      "Sample 2: setosa=0.045, versicolor=0.882, virginica=0.072\n",
      "Sample 3: setosa=0.011, versicolor=0.916, virginica=0.073\n",
      "Sample 4: setosa=0.968, versicolor=0.032, virginica=0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/PA-Venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Logistic Regression on Iris (multiclass) ===\n",
    "# Purpose:\n",
    "# - Show a clean pipeline: scaler + logistic regression.\n",
    "# - Use a stratified split, read classification report, and inspect probabilities.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "target_names = iris.target_names\n",
    "\n",
    "# Stratified split keeps class proportions similar in both sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Pipeline: Standardize features + multinomial logistic regression\n",
    "logreg_clf = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"logreg\", LogisticRegression(multi_class=\"multinomial\", max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# Train\n",
    "logreg_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_lr = logreg_clf.predict(X_test)\n",
    "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
    "\n",
    "print(\"Logistic Regression\")\n",
    "print(\"-------------------\")\n",
    "print(f\"Accuracy: {acc_lr:.3f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr, target_names=target_names))\n",
    "\n",
    "# Predicted probabilities for first 5 test examples\n",
    "proba_lr = logreg_clf.predict_proba(X_test[:5])\n",
    "print(\"Predicted probabilities for first 5 test samples:\")\n",
    "for i, probs in enumerate(proba_lr):\n",
    "    pairs = [f\"{target_names[j]}={probs[j]:.3f}\" for j in range(len(target_names))]\n",
    "    print(f\"Sample {i}: \" + \", \".join(pairs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036a68e2",
   "metadata": {},
   "source": [
    "\n",
    "**Interpretation (Logistic Regression)**\n",
    "- The classification report shows **precision**, **recall**, and **F1-score** per class.\n",
    "- Predicted probabilities illustrate how the model distributes confidence across classes.\n",
    "- Scaling helps the optimizer converge and makes coefficients more stable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca145305",
   "metadata": {},
   "source": [
    "\n",
    "## Decision Trees: Splitting Criteria and Terms\n",
    "\n",
    "A **decision tree** splits data recursively based on feature values to create purer subsets.\n",
    "\n",
    "- **Entropy**:\n",
    "$$\n",
    "H(t) = -\\sum_{i=1}^{C} p_i \\log_2(p_i).\n",
    "$$\n",
    "\n",
    "- **Information Gain**:\n",
    "$$\n",
    "IG = H(\\text{parent}) - \\sum_{k} \\frac{N_k}{N} H(\\text{child}_k).\n",
    "$$\n",
    "\n",
    "- **Gini Impurity**:\n",
    "$$\n",
    "G(t) = 1 - \\sum_{i=1}^{C} p_i^2.\n",
    "$$\n",
    "\n",
    "**Terms**\n",
    "- \\(p_i\\): proportion of class \\(i\\) at node \\(t\\).\n",
    "- \\(C\\): number of classes.\n",
    "- \\(N\\): number of samples at the parent node; \\(N_k\\): number at child \\(k\\).\n",
    "\n",
    "A good split **maximizes** information gain or **minimizes** impurity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8447de92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree (max_depth=3)\n",
      "---------------------------\n",
      "Accuracy: 0.895\n",
      "\n",
      "Feature Importances:\n",
      "petal length (cm)    0.950732\n",
      "petal width (cm)     0.049268\n",
      "sepal width (cm)     0.000000\n",
      "sepal length (cm)    0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Decision Tree Classifier (short demo) ===\n",
    "# Purpose:\n",
    "# - Show a single, interpretable tree.\n",
    "# - Display accuracy and feature importances to see what the tree relies on.\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(max_depth=3, random_state=42)  # limit depth to reduce overfitting\n",
    "dt_clf.fit(X_train, y_train)\n",
    "y_pred_dt = dt_clf.predict(X_test)\n",
    "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
    "\n",
    "print(\"Decision Tree (max_depth=3)\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"Accuracy: {acc_dt:.3f}\")\n",
    "\n",
    "# Feature importances: contribution of each feature to the tree's splits\n",
    "importances_dt = pd.Series(dt_clf.feature_importances_, index=iris.feature_names).sort_values(ascending=False)\n",
    "print(\"\\nFeature Importances:\")\n",
    "print(importances_dt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cad0786",
   "metadata": {},
   "source": [
    "\n",
    "**Interpretation (Decision Tree)**\n",
    "- A shallow tree is easier to interpret and less prone to overfitting.\n",
    "- **Feature importances** summarize which features drove the splits.\n",
    "- Trees do not require feature scaling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45fb265b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "-------------\n",
      "Accuracy: 0.895\n",
      "\n",
      "Feature Importances (RF):\n",
      "petal length (cm)    0.455831\n",
      "petal width (cm)     0.406808\n",
      "sepal length (cm)    0.112132\n",
      "sepal width (cm)     0.025229\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Random Forest Classifier ===\n",
    "# Purpose:\n",
    "# - Demonstrate an ensemble of trees that usually generalizes better than a single tree.\n",
    "# - Print accuracy and feature importances for quick interpretation.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=200, random_state=42, n_jobs=-1\n",
    ")\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred_rf = rf_clf.predict(X_test)\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"Random Forest\")\n",
    "print(\"-------------\")\n",
    "print(f\"Accuracy: {acc_rf:.3f}\")\n",
    "\n",
    "importances_rf = pd.Series(rf_clf.feature_importances_, index=iris.feature_names).sort_values(ascending=False)\n",
    "print(\"\\nFeature Importances (RF):\")\n",
    "print(importances_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0911dac8",
   "metadata": {},
   "source": [
    "\n",
    "**Interpretation (Random Forest)**\n",
    "- Ensembles average many trees and often improve test accuracy.\n",
    "- On Iris, petal measurements typically carry strong signal for separating classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbde2868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Comparison (Accuracy)\n",
      "                    Model  Accuracy\n",
      "       LogisticRegression  0.921053\n",
      "DecisionTree(max_depth=3)  0.894737\n",
      "             RandomForest  0.894737\n",
      "     Dummy(most_frequent)  0.315789\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Baseline: DummyClassifier ===\n",
    "# Purpose:\n",
    "# - Provide a \"floor\" for performance. Any useful model should beat it.\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy.fit(X_train, y_train)\n",
    "y_pred_dummy = dummy.predict(X_test)\n",
    "acc_dummy = accuracy_score(y_test, y_pred_dummy)\n",
    "\n",
    "compare_df = pd.DataFrame({\n",
    "    \"Model\": [\"Dummy(most_frequent)\", \"LogisticRegression\", \"DecisionTree(max_depth=3)\", \"RandomForest\"],\n",
    "    \"Accuracy\": [acc_dummy, acc_lr, acc_dt, acc_rf]\n",
    "}).sort_values(by=\"Accuracy\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Model Comparison (Accuracy)\")\n",
    "print(compare_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e55922",
   "metadata": {},
   "source": [
    "\n",
    "## A Note on Pipelines and Scaling\n",
    "\n",
    "- We combined **`StandardScaler`** and **`LogisticRegression`** in a **`Pipeline`** so preprocessing and model fit occur together.\n",
    "- **Scaling is typically beneficial for linear models** and distance-based methods.\n",
    "- **Trees and forests** do **not** require feature scaling; splits are threshold-based.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195f1275",
   "metadata": {},
   "source": [
    "\n",
    "## Wrap-Up\n",
    "\n",
    "- You trained and interpreted **Logistic Regression**, **Decision Tree**, and **Random Forest** on Iris.\n",
    "- You examined **accuracy**, **classification reports**, **predicted probabilities**, and **feature importances**.\n",
    "- A **dummy baseline** set a reference point for model performance.\n",
    "\n",
    "**Next Module:** Model evaluation and optimization (metrics beyond accuracy, proper validation, cross-validation, and regularization).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PA-Venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
